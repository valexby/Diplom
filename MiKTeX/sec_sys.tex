\section{СИСТЕМНОЕ ПРОЕКТИРОВАНИЕ}\label{sec:sys}
Изучив теоретические аспекты разрабатываемого приложения и выработав список требований необходимых для разработки модуля, разбиваем систему на компоненты.
В разрабатываемом приложении можно выделить следующие блоки:
\begin{itemize}
\item модуль сбора статистики;
\item модуль объектно-реляционного отображения;
\item база данных объектов для анализа;
\item модуль развертки;
\item тренировочный набор Стенфорда;
\item модуль выделения особенностей;
\item модуль модели анализатора;
\item модуль ячейки Tree LSTM\@;
\item визуализации CoreNLP\@;
\item модуль визуализации TensorBoard.
\end{itemize}
Структурная схема, иллюстрирующая перечисленные блоки и связи между ними приведена на чертеже ГУИР.400201.009 C1.


\subsection{Подготовка данных}\label{subsec:sys:data_gathering}
Чтобы начать работу модели распознавания тональностей, необходимы входные данные. Первый тип входных данных --- это тренировочный набор Стенфорда. Он необходим для обучения модели распознаванию тональностей. Стенфордский набор Представляет собой выборку синтаксических деревьев составляющих. Однако модель должна работать с синтаксическими деревьями зависимостей. Поэтому необходимо привести его к виду синтаксических деревьев зависимостей. Так как исследование синтаксического разбора не входит в задачи проектирования, то для этой цели будет использоваться синтаксический анализатор из CoreNLP\@. Он не является самым производительным, но самым простым в применении. За несколько секунд он обрабатывает более 10000 предложений, составляющих Стенфордский тренировочный набор. Обработать тренировочный набор необходимо всего один, при установке программы, поэтому скорость в несколько секунд вполне удовлетворительна. Объем анализатора вместе с моделью нейросети, обученной синтаксическому разбору, составляет более 350 мегабайт. Далее нужен предобученный набор встроенных векторов GloVe\@. Он содержит в себе 5 гигабайт слов и соответствующих им векторам размерностью в 300 составляющих каждый. Чтобы сократить размер, занимаемый набором GloVe\@, он будет отфильтрован: будут выброшены все слова, которые не встречаются в Стенфордском тренировочном наборе. После такой фильтрации набор встраиваемых векторов будет занимать около 30 мегабайт дискового пространства. Фильтрацию так же необходимо произвести только один раз при установке программы. То есть, если перед началом работы программы произвести предобработку входных данных, то это повысит производительность и сократить объем, занимаемый системой на диске. Итак, чтобы решать проблему управления зависимостями, введен модуль развертки. В его задачи входит:

\begin{itemize}
\item проверить наличие отфильтрованного набора GloVe;
\item проверить наличие Стенфорского тренировочного набора в виде синтаксических деревьев зависимости;
\item проверить наличие синтаксического анализатора и скачать его, если отсутствует;
\item при необходимости скачать оригинальные набор GloVe и тренировочный набор;
\item при необходимости отфильтровать GloVe и удалить оригинальную версию;
\item при необходимости обработать оригинальный Стенфордский тренировочный набор с помощью синтаксического анализатора;
\item проверить наличием необходимых библиотек и установить отсутствующие.
\end{itemize}

В результате работы модуля развертки, модель будет способна обучаться. Для того чтобы расширить возможности исследования возможностей модели, введен модуль сбора статистики. Его задачей будет сбор данных с сайта rottentomatoes.com, популярного форума о кино. Там собрано множество отзывов о фильмах последних десятилетий. Так же он предоставляет API для разработчиков, что поможет в сборе данных с ресурса. Для демонстрации хватит отзывов профессиональных критиков и обычных пользователей, и рейтинга фильма. Собранные через API данные необходимо хранить в базе данных, так как сбор данных для анализа может занять значительно больше времени, чем сам анализ. А база данных поможет сохранить результаты, и продолжить сбор информации в любое удобное время.

Так как сбор данных через API --- задача достаточно тривиальная, в которой производительность зависит от скорости обмена данными с сервером, то производительность языка Python даст удовлетворительные результаты. Так же Python скриптовый язык, что упростит разработку. Для работы с REST API отлично подойдет библиотека requests для Python, которая просто представят собой реализацию REST-интереса и довольно проста в использовании.

В качестве базы данных будет использована Mysql, как стабильное и надежное решение. Для удобства работа с базой данных будет организована по принципу объектно-реляционного отображения. Данный принцип предполагает абстракцию модели транзакций и предлагает программную модель --- объекты базы данных представляются в виде объектов в модели объектно-ориентированного программирования. Для реализации объектно-реляционного отображения будет использована библиотека sqlalchemy, которая позволяет используя возможности языка Python максимально абстрагироваться от SQL\@. В объектах sqlalchemy будут хранится данные из базы данных. И любые изменения этих объектов sqlalchemy представляет в виде транзакции в базу, таким образом поддерживая актуальность данных в базе и в памяти программы.

Таким образом сбор данных для NLP-модели будет осуществляться модулем сбора статистики через API ресурса rottentomatoes.com. Полученные от сайта объекты REST будут сохраняться в объектах Python, которые в свою очередь, являясь объектом-реляционным отображениями базы данных, будут управлять данными в базе. И затем, когда данные из базы данных потребуется проанализировать, то они будут загружены в виде Python-объектов в рамках той же объектно-реляционной модели.

\subsection{Анализ данных}\label{subsec:sys:data_analysis}
В модели нейронной сети может протекать два процесса: обучение и анализ. Обучение сети выполняется с помощью тренировочного набора Стенфорда. А анализ происходит на обученной сети с данными, собранными модулем сбора статистики. Процесс обучения очень сложен в плане вычислений и требует много времени и ресурсов. Поэтому существует возможность сохранения обученной сети в файл и последующей загрузки из файла. Однако для исследовательских целей контроль обучения модели очень важен. К тому же обучение модели Tree LSTM, за счет сложности архитектуры, происходит намного быстрее множества других нейронных сете. Процесс анализа же очень прост, так так математически нейронная сеть это линейная функция, и алгоритм анализа имеет линейную сложность.

Модуль выделения особенностей в процессе обучения модели выполняет синтаксический разбор тренировочного набора Стенфорда, что описано выше. В результате полученные деревья в виде списков Python сохраняются в формате JSON\@. В результате получается три набора: тренировочный, тестовый и набор разработки, каждый из которых составляет соответственно 10\%, 10\% и 80\% от всего набора Стенфорда. В процессе анализа данный модуль производит синтаксический разбор предложений, подлежащий обработке. И точно так же в JSON формате подается в модуль модели анализатора.

Модуль анализатора и модуль ячейки Tree LSTM представляют собой непосредственно саму нейронную сеть как математический алгоритм. В качестве основного инструмента реализации сети выбор пал на Tensorflow. В машинном обучении Tensorflow является стандартом на сегодняшний день. Данный фреймворк имеет API различных уровней абстракций: с использованием низкого уровня можно реализовывать модель в виде последовательности операций линейной алгебры, а на высоком уровне составными частями алгоритма являются слои нейронных сетей. Так же tensorflow предоставляет возможности выполнения вычислений на графических процессорах, и так же на тензорных процессорах (TPU).
Кроме того, tensorflow представил в конце 2017го технологию \textit{горячего выполнения}. Так как в классической архитектуре ПК оперативная память и память графического процессора разделены, и центральный процессор не имеет прямого доступа к видеопамяти, то программирование алгоритмов на графических процессорах вызывает сложности. Процесс написания кода на GPU с использованием популярных технологий, таких как CUDA, Torch, OpenCV и Tensorflow в том числе, заключается в предварительном построении графа выполнение, узлы которого --- это операции GPU, а грани --- это движение данных. И результат промежуточных вычислений недоступен, только выход всего графа целиком. Горячее вычисление предоставляет возможность получить промежуточные результаты без серьезных потерь производительности. Это во многом упрощает процесс отладки. В рамках данного проекта была построена модель с использованием этой техники. На данный момент в открытом доступе очень немного реализаций алгоритмов машинного обучения с горячими вычислениями.

Итак, модуль ячейки Tree LSTM представляет собой реализацию ячейки Child Sum Tree LSTM. Реализована она в виде класса наследника tensorflow.keras.Model --- базового элемента высокоуровнего API keras. То есть, ячейка Tree LSTM является полноценным слоем в рамках tensorflow, и, что интересно в данной работе, поддерживает автоматический расчет градиента.

Модуль анализатора же не является классом keras, так как принимает на вход синтаксические деревья. Объекты tensorflow созданы для того, чтобы работать и на CPU и на GPU, поэтому обрабатывают они только тензоры, которые легко хранить в графической памяти. Дерево же довольно сложная структура, чтобы представить ее в виде тензора. Таким образом, модуль анализатора содержит в себе множество слоев нейронных сетей:

\begin{itemize}
\item слой проекции встроенных векторов;
\item слой исключения;
\item слой Child Sum Tree LSTM;
\item слой линейной регрессии.
\end{itemize}

Каждый узел синтаксического дерева будет проходить последовательно все эти слои в процессе обучения модели. В процессе выполнения слой регрессии будет выполнятся только в корне дерева.

Процесс анализа обученной моделью некоторого дерева начинается с подачи дерева на вход модели. Модель рекурсивно спускается к листьям дерева и выполняет последовательно слои модели на каждом узле. В результате работы на выходе линейной регрессии получается вектор, элементами которого являются вероятности принадлежности входного дерева каждому из целевых классов. Индекс элемента с наибольшей вероятностью и будет номером класса, в чью пользу модель сделала выбор.

Процесс обучения включает в себя многократное выполнение процесса анализа. Для обучения сети нужны тренировочный набор деревьев и набор разработки. Сам процесс обучения делится на эпохи. В ходе эпохи модель обрабатывает тренировочный набор целиком. В начале каждой эпохи элементы тренировочного набора перемешиваются в случайном порядке. Далее набор делится на равные группы, за исключением последней, если количество деревьев в наборе не кратно размеру группы. Каждая группа последовательно анализируется моделью, для каждого анализа по принципу обратной связи вычисляется функция потерь. Потери для деревьев в группе суммируются и после того, как группа целиком обработана, вычисляется градиент сети относительно каждого обучаемого параметра модели, куда в качестве аргумента передается суммарная потеря. Обучаемыми параметрами являются коэффициенты функций, которыми описаны различные слои нейронной сети. После этого, результаты градиента относительно каждого из обучаемых параметров сети добавляются к значениям этого же параметра.

После того, как обработаны все группы, эпоха считается законченной. Однако, помимо обучаемых параметров, модель имеет ещё и гипер-параметры --- параметры, которые не возможно эффективно обучить методом градиентного спуска: например количество эпох, размер группы, количество элементов скрытого слоя того или иного слоя нейронной сети. Однако данные параметры играют очень важную роль в обучении сети и их успешный подбор зависит от опыта инженера. Для оценки успешности выбранных гипер-параметров, в конце каждой эпохи обучения производится анализ деревьев из набора разработки, и высчитывается оценка точности. Гипер-параметры изменяются для получения наилучшего результата точности на наборе разработки.

После того, как все эпохи обучения пройдены, модель анализирует деревья из тестового набора. Результирующая точность является качественной оценкой нейронной сети. Подбор гипер-параметров по тестовой выборке, вместо выборки разработки крайне не рекомендуется, так как может привести к переобучению сети, и результаты точности, полученные на тестовой выборке, могут очень сильно отличатся от результатов полученных в ходе практического применения сети.

Модуль визуализации CoreNLP представляет собой небольшой скрипт на JavaScript, который наглядно показывает, как считалась оценка тональности, и поможет понять, почему именно алгоритм сделал такой вывод. Это позволит выявить проблемы отсутствия какого-то ключевого слова в наборе векторов GloVe.

Модуль визуализации TensoBorad демонстрирует процесс изменения весов во время обучения, что является очень интересной информацией для исследований.